# Streaming en el Frontend con Next.js

Este proyecto implementa streaming en tiempo real en el frontend usando Next.js API Routes y React.

## üèóÔ∏è Arquitectura

### 1. **API Route** (`/api/stream`)

- **Ubicaci√≥n**: `src/app/api/stream/route.ts`
- **Funci√≥n**: Recibe mensajes y devuelve un stream de respuesta
- **M√©todo**: POST
- **Formato**: Server-Sent Events (SSE)

### 2. **Componente React** (`StreamingChat`)

- **Ubicaci√≥n**: `src/components/StreamingChat.tsx`
- **Funci√≥n**: Interfaz de usuario para consumir el stream
- **Caracter√≠sticas**:
  - Streaming en tiempo real
  - Bot√≥n de cancelar
  - Manejo de errores
  - Indicador de carga

## üöÄ C√≥mo Funciona

### Flujo de Datos:

1. **Usuario** escribe mensaje en el frontend
2. **Frontend** env√≠a POST a `/api/stream`
3. **API Route** procesa con Google Generative AI
4. **Stream** se env√≠a chunk por chunk al frontend
5. **Frontend** muestra tokens en tiempo real

### C√≥digo Clave:

#### API Route:

```typescript
const stream = new ReadableStream({
  async start(controller) {
    const responseStream = await model.stream(messages);

    for await (const chunk of responseStream) {
      const content = processChunk(chunk.content);
      controller.enqueue(
        encoder.encode(`data: ${JSON.stringify({ content })}\n\n`)
      );
    }

    controller.close();
  },
});
```

#### Frontend:

```typescript
const reader = response.body?.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  buffer += decoder.decode(value, { stream: true });
  // Procesar chunks y actualizar UI
}
```

## üéØ Caracter√≠sticas Implementadas

### ‚úÖ **Streaming en Tiempo Real**

- Los tokens aparecen uno por uno
- Experiencia similar a ChatGPT

### ‚úÖ **Cancelaci√≥n de Requests**

- Bot√≥n para cancelar requests en progreso
- Limpieza autom√°tica de recursos

### ‚úÖ **Manejo de Errores**

- Try-catch en API y frontend
- Mensajes de error amigables

### ‚úÖ **Prompts del Sistema**

- Soporte para prompts personalizados
- Conversaciones estructuradas

### ‚úÖ **UI Responsiva**

- Dise√±o adaptativo
- Indicadores de estado
- Animaciones suaves

## üîß Uso

### 1. **Ejecutar el Proyecto**:

```bash
npm run dev
```

### 2. **Acceder al Chat**:

- Ve a `http://localhost:3000`
- Busca la secci√≥n "Chat con Streaming en Tiempo Real"

### 3. **Usar el Streaming**:

- Escribe tu mensaje
- Presiona "Enviar"
- Ve la respuesta aparecer token por token
- Usa "Cancelar" si quieres parar

## üìù Personalizaci√≥n

### Cambiar el Prompt del Sistema:

```typescript
<StreamingChat systemPrompt="Eres un experto en programaci√≥n que explica conceptos t√©cnicos de manera simple." />
```

### Agregar Nuevas Funcionalidades:

1. **Historial de conversaci√≥n**
2. **M√∫ltiples modelos**
3. **Configuraci√≥n de temperatura**
4. **Exportar conversaciones**

## üêõ Troubleshooting

### Error: "No se pudo leer el stream"

- Verifica que la API key est√© configurada
- Revisa la consola del navegador

### Error: "Request cancelado"

- Normal cuando presionas cancelar
- No es un error real

### Streaming lento:

- Verifica tu conexi√≥n a internet
- El modelo puede tardar en responder

## üîÆ Pr√≥ximas Mejoras

- [ ] Historial de conversaciones
- [ ] M√∫ltiples modelos de IA
- [ ] Configuraci√≥n de par√°metros
- [ ] Exportar conversaciones
- [ ] Modo oscuro/claro
- [ ] Soporte para archivos
